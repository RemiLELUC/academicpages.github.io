---
title: "Asymptotic Optimality of Conditioned Stochastic Gradient Descent (preprint)"
collection: publications
permalink: /publication/csgd
excerpt: "With F.Portier. ([PDF](https://arxiv.org/pdf/2006.02745.pdf))"
# date: '2019-11-19'
# authors: "R.Leluc, F.Portier"
---
<p align="justify">
In this paper, we investigate a general class of stochastic gradient descent (SGD) algorithms, called __conditioned__ SGD, based on a preconditioning of the gradient direction. Using a discrete-time approach with martingale tools, we establish the weak convergence of the rescaled sequence of iterates for a broad class of conditioning matrices. In particular, when the conditioning matrix is an estimate of the inverse Hessian at the optimal point, the algorithm is proved to be asymptotically optimal. Finally, we provide a practical procedure to achieve this minimum variance and validate this method on simulated and real datasets. 
